{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c54126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf8\n",
    "\"\"\"\n",
    "# Author : Jianbai(Gus) Ye\n",
    "# created at Feb 2 2019\n",
    "# pytorch implementation of HSIC bottleneck method\n",
    "# reference : https://github.com/forin-xyz/Keras-HSIC-Bottleneck\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from utils import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca142c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756a4a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HSICBottleneck:\n",
    "    def __init__(self, args):\n",
    "        self.model      = MLP(args)\n",
    "        self.model.to(device)\n",
    "        self.batch_size = args.batchsize\n",
    "        self.lambda_0   = args.lambda_\n",
    "        self.sigma      = args.sigma_\n",
    "        self.extractor  = 'hsic'\n",
    "        self.last_linear = \"output_layer\"\n",
    "        self.HSIC = compute_HSIC(args.HSIC)\n",
    "        self.kernel = compute_kernel()\n",
    "        self.kernel_forward = args.kernel_forward\n",
    "        self.kernel_backward = args.kernel_backward\n",
    "        self.forward = args.forward\n",
    "        \n",
    "        self.opt = optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "        self.iter_loss1, self.iter_loss2, self.iter_loss3 = [], [], []\n",
    "        self.track_loss1, self.track_loss2, self.track_loss3 = [], [], []\n",
    "        \n",
    "        self.loss = args.loss\n",
    "        if self.loss == \"mse\": self.output_criterion = nn.MSELoss()\n",
    "        elif self.loss == \"CE\": self.output_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def step(self, input_data, labels):\n",
    "        \n",
    "        self.model.train()\n",
    "        labels_float = F.one_hot(labels, num_classes=10).float()\n",
    "        \n",
    "        if self.forward == \"x\": Kx  = self.kernel(input_data, self.sigma, self.kernel_forward)\n",
    "        Ky = self.kernel(labels_float, self.sigma, self.kernel_backward) \n",
    "        y_pred, hidden_zs = self.model(input_data)\n",
    "        \n",
    "        total_loss1, total_loss2, total_loss3 = 0., 0., 0.\n",
    "        kernel_list = list()\n",
    "        for num, feature in enumerate(hidden_zs): kernel_list.append(self.kernel(feature, self.sigma, self.kernel_forward))\n",
    "        \n",
    "        for num, feature in enumerate(kernel_list):\n",
    "            if num == (len(hidden_zs)-1): \n",
    "                loss1 = self.HSIC(feature, kernel_list[num-1], self.batch_size, device)\n",
    "                if self.loss == \"mse\": total_loss3 += self.output_criterion(hidden_zs[-1], labels_float)\n",
    "                if self.loss == \"CE\": total_loss3 += self.output_criterion(hidden_zs[-1], labels)\n",
    "            elif num == 0:\n",
    "                if self.forward == \"x\": loss1 = self.HSIC(feature, Kx, self.batch_size, device)\n",
    "                loss2 = - self.lambda_0*self.HSIC(feature, Ky, self.batch_size, device)\n",
    "            else:\n",
    "                if self.forward == \"f\": loss1 = self.HSIC(feature, kernel_list[num-1], self.batch_size, device)\n",
    "                elif self.forward == \"x\": loss1 = self.HSIC(feature, Kx, self.batch_size, device)\n",
    "                loss2 = - self.lambda_0*self.HSIC(feature, Ky, self.batch_size, device)\n",
    "            if self.forward == \"f\" or self.forward == \"x\": total_loss1 += loss1\n",
    "            total_loss2 += loss2\n",
    "        \n",
    "        if self.forward == \"f\" or self.forward == \"x\": \n",
    "            total_loss = total_loss1 + total_loss2 + total_loss3\n",
    "            self.iter_loss1.append(total_loss1.item())\n",
    "        if self.forward == \"n\": \n",
    "            total_loss = total_loss2 + total_loss3\n",
    "            self.iter_loss1.append(-1)\n",
    "        self.opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.opt.step()\n",
    "                \n",
    "        self.iter_loss2.append(total_loss2.item())\n",
    "        self.iter_loss3.append(total_loss3.item())\n",
    "        \n",
    "    def update_loss(self):\n",
    "        self.track_loss1.append(np.mean(self.iter_loss1))\n",
    "        self.track_loss2.append(np.mean(self.iter_loss2))\n",
    "        self.track_loss3.append(np.mean(self.iter_loss3))\n",
    "        self.iter_loss1, self.iter_loss2, self.iter_loss3 = [], [], []\n",
    "    \n",
    "    def tune_output(self, input_data, labels):\n",
    "        self.model.train()\n",
    "        if self.loss == \"mse\":\n",
    "            one_hot_labels = F.one_hot(labels, num_classes=10)\n",
    "            labels = F.one_hot(labels, num_classes=10).float()\n",
    "        \n",
    "        y_pred, hidden_zs = self.model(input_data)\n",
    "        total_loss = self.output_criterion(hidden_zs[-1], labels)\n",
    "        self.opt.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.opt.step()\n",
    "    \n",
    "def show_result():\n",
    "    hsic.model.eval()\n",
    "    with torch.no_grad():\n",
    "        counts, correct, counts2, correct2 = 0, 0, 0, 0        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader): \n",
    "            output = hsic.model.forward(data.view(batch_size, -1).to(device))[0].cpu()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += (pred[:,0] == target).float().sum()\n",
    "            counts += len(pred)\n",
    "        for batch_idx, (data, target) in enumerate(test_loader): \n",
    "            output = hsic.model.forward(data.view(batch_size, -1).to(device))[0].cpu()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct2 += (pred[:,0] == target).float().sum()\n",
    "            counts2 += len(pred)\n",
    "        print(\"Training  ACC: {:.4f} \\t Testing ACC: {:.4f}\".format(correct/counts, correct2/counts2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef0233-e3a3-459f-a3b1-b5984b879794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93683dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "Training  ACC: 0.7031 \t Testing ACC: 0.7027\n",
      "5.86\n",
      "EPOCH 10\n",
      "Training  ACC: 0.9259 \t Testing ACC: 0.9254\n",
      "47.25\n",
      "EPOCH 20\n",
      "Training  ACC: 0.9299 \t Testing ACC: 0.9226\n",
      "47.34\n",
      "EPOCH 30\n",
      "Training  ACC: 0.9358 \t Testing ACC: 0.9299\n",
      "47.70\n",
      "EPOCH 40\n",
      "Training  ACC: 0.9359 \t Testing ACC: 0.9268\n",
      "68.92\n",
      "EPOCH 50\n",
      "Training  ACC: 0.9358 \t Testing ACC: 0.9256\n",
      "67.60\n",
      "EPOCH 60\n",
      "Training  ACC: 0.9392 \t Testing ACC: 0.9283\n",
      "47.57\n",
      "EPOCH 70\n",
      "Training  ACC: 0.9403 \t Testing ACC: 0.9290\n",
      "47.69\n",
      "EPOCH 80\n",
      "Training  ACC: 0.9375 \t Testing ACC: 0.9248\n",
      "69.60\n",
      "EPOCH 90\n",
      "Training  ACC: 0.9394 \t Testing ACC: 0.9247\n",
      "82.21\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--loss', type=str, default=\"CE\")\n",
    "    parser.add_argument('--HSIC', type=str, default=\"nHSIC\")\n",
    "    parser.add_argument('--kernel_forward', type=str, default=\"rbf\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--kernel_backward', type=str, default=\"student\", choices=[\"rbf\", \"student\"])\n",
    "    parser.add_argument('--sigma_', type=int, default=1)\n",
    "    parser.add_argument('--lambda_', type=int, default=100)\n",
    "    parser.add_argument('--batchsize', type=int, default=256)\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "    parser.add_argument('--bn_affine', type=int, default=0)\n",
    "    parser.add_argument('--forward', type=str, default=\"n\", choices=[\"x\", \"f\", \"n\"])\n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    device = \"cuda:{}\".format(args.device)\n",
    "    batch_size = args.batchsize\n",
    "    train_loader, test_loader = load_data(batch_size=args.batchsize)\n",
    "    \n",
    "    hsic = HSICBottleneck(args)\n",
    "    start = time.time()\n",
    "    for epoch in range(100):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.view(batch_size, -1)\n",
    "            hsic.step(data.view(batch_size, -1).to(device), target.to(device))\n",
    "            hsic.tune_output(data.view(batch_size, -1).to(device), target.to(device))\n",
    "        if epoch in range(0, 100, 10):\n",
    "            print(\"EPOCH %d\" % epoch)\n",
    "            show_result()\n",
    "            print(\"{:.2f}\".format(time.time()-start))\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b5032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT17",
   "language": "python",
   "name": "pt17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
